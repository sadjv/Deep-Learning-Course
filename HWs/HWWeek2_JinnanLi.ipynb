{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Programming Assignment \n",
    "\n",
    "Remark: \n",
    "\n",
    "Please upload your solutions of this assignment to Canvas with a file named \"Programming_Assignment_2 _yourname.ipynb\" before 11:59pm May 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2ENBj8Nz3kg"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8Op_NP5z3kf"
   },
   "source": [
    "### **Problem 1 (4 pt).** Use stochastic gradient descent method to train MNIST with the logisitc regression model to achieve at least 92% test accuracy. Print the results with the following format:\n",
    "\n",
    "   \"Epoch: i, Training accuracy: $a_i$, Test accuracy: $b_i$\"\n",
    "\n",
    "where $i=1,2,3,...$ means the $i$-th epoch,  $a_i$ and $b_i$ are the training accuracy and test accuracy computed at the end of $i$-th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], train accuracy: 0.8810166666666667, test accuracy: 0.9105\n",
      "Epoch [2], train accuracy: 0.90835, test accuracy: 0.9156\n",
      "Epoch [3], train accuracy: 0.9143166666666667, test accuracy: 0.9181\n",
      "Epoch [4], train accuracy: 0.9178333333333333, test accuracy: 0.9211\n",
      "Epoch [5], train accuracy: 0.9195, test accuracy: 0.9211\n",
      "Epoch [6], train accuracy: 0.9206166666666666, test accuracy: 0.9232\n",
      "Epoch [7], train accuracy: 0.9218666666666666, test accuracy: 0.9227\n",
      "Epoch [8], train accuracy: 0.9235333333333333, test accuracy: 0.9228\n",
      "Epoch [9], train accuracy: 0.9235166666666667, test accuracy: 0.9226\n",
      "Epoch [10], train accuracy: 0.9237666666666666, test accuracy: 0.9218\n",
      "Epoch [11], train accuracy: 0.92495, test accuracy: 0.924\n",
      "Epoch [12], train accuracy: 0.9259, test accuracy: 0.924\n",
      "Epoch [13], train accuracy: 0.9262, test accuracy: 0.925\n",
      "Epoch [14], train accuracy: 0.9260666666666667, test accuracy: 0.9227\n",
      "Epoch [15], train accuracy: 0.9269666666666667, test accuracy: 0.9252\n",
      "Epoch [16], train accuracy: 0.9274666666666667, test accuracy: 0.9257\n",
      "Epoch [17], train accuracy: 0.9272833333333333, test accuracy: 0.9239\n",
      "Epoch [18], train accuracy: 0.9281333333333334, test accuracy: 0.924\n",
      "Epoch [19], train accuracy: 0.9280333333333334, test accuracy: 0.9242\n",
      "Epoch [20], train accuracy: 0.9284, test accuracy: 0.9251\n"
     ]
    }
   ],
   "source": [
    "# write your code for solving probelm 1 in this cell\n",
    "# Hint: you can tune the number of epoches, learning rate and mini-batch size.\n",
    "import torch\n",
    "import torch.nn as nn # Import functions from torch.nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "def model(input_size,num_classes):\n",
    "    return nn.Linear(input_size,num_classes)\n",
    "\n",
    "input_size = 784 #1*28*28=784\n",
    "num_classes = 10 #10\n",
    "\n",
    "\n",
    "my_model =model(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=0.1)\n",
    "\n",
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train= True, download=True, transform=MNIST_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train= False, download=True, transform=MNIST_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False) \n",
    "# Write a loop to train the model using the given optimizer and loss functions.\n",
    "num_epochs = 20\n",
    "# One epoch is when an entire dataset is passed through the neural network only once.\n",
    "for epoch in range(num_epochs):\n",
    "    num_train = 0\n",
    "    total_tr = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # You can check the size of image before images.reshape, which is [batch_size,1,28,28]\n",
    "        #print(images.size()) \n",
    "        # Reshape MNIST images to (batch_size, input_size)\n",
    "        images = images.reshape(images.size(0), 28*28)\n",
    "        \n",
    "        # Forward pass to get the loss\n",
    "        outputs = my_model(images) # the outputs of xW^{T}+b\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  #backpropragation\n",
    "        optimizer.step()\n",
    "        \n",
    "        p_max1, predicted1 = torch.max(outputs, 1) \n",
    "        total_tr += labels.size(0) # you can also use labels.size()[0], see the test  below\n",
    "        num_train += (predicted1 == labels).sum()\n",
    "        \n",
    "    num_test = 0\n",
    "    total_te = 0\n",
    "    for i, (images, labels) in enumerate(testloader):\n",
    "        images = images.reshape(images.size(0), 1*28*28)\n",
    "        outputs = my_model(images)\n",
    "        \n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total_te += labels.size(0) # you can also use labels.size()[0], see the test  below\n",
    "        num_test += (predicted == labels).sum()\n",
    "        \n",
    "    print('Epoch [{}], train accuracy: {}, test accuracy: {}' .format(epoch+1,float(num_train)/total_tr,float(num_test)/total_te))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2ENBj8Nz3kg"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2 (6 pts).** Extract the subset of data which are labeled with 0,1,3,4,7 from MNIST dataset. Use both full gradient descent method and stochastic gradient descent method to train this subset with the logisitc regression model to achieve the training and test accuracy as high as possible. Print the results with the following format:\n",
    "\n",
    "* For full gradient descent method, print:\n",
    "\n",
    "    \"Full gradient descent, Epoch: i, Training accuracy: $a_i$, Test accuracy: $b_i$\"\n",
    "\n",
    "\n",
    "* For stochastic gradient descent method, print:\n",
    "\n",
    "    \"Stochastic gradient descent, Epoch: i, Training accuracy: $a_i$, Test accuracy: $b_i$\"\n",
    "\n",
    "where $i=1,2,3,...$ means the $i$-th epoch,  $a_i$ and $b_i$ are the training accuracy and test accuracy computed at the end of $i$-th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For full gradient descent method:\n",
      "Epoch [1], train accuracy: 0.961460052422095, test accuracy: 0.9698149951314509\n",
      "Epoch [2], train accuracy: 0.9719121120926771, test accuracy: 0.9626095423563777\n",
      "Epoch [3], train accuracy: 0.97430670161473, test accuracy: 0.9678675754625121\n",
      "Epoch [4], train accuracy: 0.9748891693363104, test accuracy: 0.9777994157740993\n",
      "Epoch [5], train accuracy: 0.9762159013687991, test accuracy: 0.9631937682570594\n",
      "Epoch [6], train accuracy: 0.9767336504546484, test accuracy: 0.9777994157740993\n",
      "Epoch [7], train accuracy: 0.9776073520370191, test accuracy: 0.978188899707887\n",
      "Epoch [8], train accuracy: 0.9772513995404977, test accuracy: 0.9764362220058422\n",
      "Epoch [9], train accuracy: 0.9790958806588357, test accuracy: 0.9762414800389484\n",
      "Epoch [10], train accuracy: 0.9795165517910882, test accuracy: 0.9686465433300876\n",
      "Epoch [11], train accuracy: 0.9785781315729865, test accuracy: 0.9700097370983447\n",
      "Epoch [12], train accuracy: 0.9787075688444488, test accuracy: 0.978188899707887\n",
      "Epoch [13], train accuracy: 0.9799048636054752, test accuracy: 0.9727361246348588\n",
      "Epoch [14], train accuracy: 0.9807462058699803, test accuracy: 0.9760467380720546\n",
      "Epoch [15], train accuracy: 0.980843283823577, test accuracy: 0.9764362220058422\n",
      "Epoch [16], train accuracy: 0.9812639549558295, test accuracy: 0.9727361246348588\n",
      "Epoch [17], train accuracy: 0.9812315956379639, test accuracy: 0.9750730282375852\n",
      "Epoch [18], train accuracy: 0.9820082192667379, test accuracy: 0.9779941577409932\n",
      "Epoch [19], train accuracy: 0.9807138465521147, test accuracy: 0.9774099318403116\n",
      "Epoch [20], train accuracy: 0.9818140633595444, test accuracy: 0.9766309639727361\n",
      "done the process\n",
      "\n",
      "\n",
      "For stochastic gradient descent method:\n",
      "Epoch [1], train accuracy: 0.9861178526356664, test accuracy: 0.9787731256085687\n",
      "Epoch [2], train accuracy: 0.9875416626217519, test accuracy: 0.9791626095423563\n",
      "Epoch [3], train accuracy: 0.988383004886257, test accuracy: 0.9793573515092503\n",
      "Epoch [4], train accuracy: 0.988771316700644, test accuracy: 0.9793573515092503\n",
      "Epoch [5], train accuracy: 0.9894185030579555, test accuracy: 0.9791626095423563\n",
      "Epoch [6], train accuracy: 0.9897097369187458, test accuracy: 0.9795520934761441\n",
      "Epoch [7], train accuracy: 0.990000970779536, test accuracy: 0.9793573515092503\n",
      "Epoch [8], train accuracy: 0.9901951266867295, test accuracy: 0.979746835443038\n",
      "Epoch [9], train accuracy: 0.9901951266867295, test accuracy: 0.979746835443038\n",
      "Epoch [10], train accuracy: 0.990227486004595, test accuracy: 0.9795520934761441\n",
      "Epoch [11], train accuracy: 0.9903569232760573, test accuracy: 0.9799415774099318\n",
      "Epoch [12], train accuracy: 0.9902922046403262, test accuracy: 0.979746835443038\n",
      "Epoch [13], train accuracy: 0.9905187198653852, test accuracy: 0.979746835443038\n",
      "Epoch [14], train accuracy: 0.9905187198653852, test accuracy: 0.9803310613437196\n",
      "Epoch [15], train accuracy: 0.9908746723619066, test accuracy: 0.9801363193768257\n",
      "Epoch [16], train accuracy: 0.9906805164547131, test accuracy: 0.9801363193768257\n",
      "Epoch [17], train accuracy: 0.9909070316797722, test accuracy: 0.9803310613437196\n",
      "Epoch [18], train accuracy: 0.9910688282691, test accuracy: 0.9799415774099318\n",
      "Epoch [19], train accuracy: 0.9909717503155033, test accuracy: 0.9801363193768257\n",
      "Epoch [20], train accuracy: 0.9910364689512345, test accuracy: 0.9801363193768257\n",
      "done the process\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn # Import functions from torch.nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "def model(input_size,num_classes):\n",
    "    return nn.Linear(input_size,num_classes)\n",
    "\n",
    "input_size = 784 #1*28*28=784\n",
    "num_classes = 10\n",
    "\n",
    "my_model =model(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=0.1)\n",
    "\n",
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train= True, download=True, transform=MNIST_transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train= False, download=True, transform=MNIST_transform)\n",
    "\n",
    "def get_indices(dataset):\n",
    "    indices =  []\n",
    "    for j in [0,1,3,4,7]:\n",
    "        for i in range(len(dataset.targets)):\n",
    "            if (dataset.targets[i] == j):\n",
    "                indices.append(i)\n",
    "    return indices\n",
    "\n",
    "idx = get_indices(trainset)\n",
    "idy = get_indices(testset)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,sampler =torch.utils.data.sampler.SubsetRandomSampler(idx))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, sampler =torch.utils.data.sampler.SubsetRandomSampler(idy))\n",
    "\n",
    "# Write a loop to train the model using the given optimizer and loss functions.\n",
    "num_epochs = 20\n",
    "# One epoch is when an entire dataset is passed through the neural network only once.\n",
    "def opti(epoch_size,trainloader,testloader):\n",
    "    for epoch in range(epoch_size):\n",
    "        num_train = 0\n",
    "        total_tr = 0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            images = images.reshape(images.size(0), 28*28)\n",
    "        \n",
    "        # Forward pass to get the loss\n",
    "            outputs = my_model(images) # the outputs of xW^{T}+b\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and compute the gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  #backpropragation\n",
    "            optimizer.step()\n",
    "        \n",
    "            p_max1, predicted1 = torch.max(outputs, 1) \n",
    "            total_tr += labels.size(0) # you can also use labels.size()[0], see the test  below\n",
    "            num_train += (predicted1 == labels).sum()\n",
    "        \n",
    "        num_test = 0\n",
    "        total_te = 0\n",
    "        for i, (images, labels) in enumerate(testloader):\n",
    "            images = images.reshape(images.size(0), 1*28*28)\n",
    "            outputs = my_model(images)\n",
    "        \n",
    "            p_max, predicted = torch.max(outputs, 1) \n",
    "            total_te += labels.size(0) # you can also use labels.size()[0], see the test  below\n",
    "            num_test += (predicted == labels).sum()\n",
    "        \n",
    "        print('Epoch [{}], train accuracy: {}, test accuracy: {}' .format(epoch+1,float(num_train)/total_tr,float(num_test)/total_te))\n",
    "    return('done the process')\n",
    "\n",
    "print('For full gradient descent method:')\n",
    "print(opti(num_epochs,trainloader,testloader))\n",
    "print('\\n')\n",
    "trainloaderb = torch.utils.data.DataLoader(trainset, batch_size=100,sampler =torch.utils.data.sampler.SubsetRandomSampler(idx))\n",
    "print('For stochastic gradient descent method:')\n",
    "print(opti(num_epochs,trainloaderb,testloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "A8btbBuFz3j_",
    "8tnDF8N5z3j_",
    "6NGxntKLz3kO",
    "t8Op_NP5z3kf",
    "eY74kXCSz3kg",
    "PsX9sU1Nz3kh",
    "tt6oay6Rz3kk",
    "kTHHlHSbz3kn",
    "O-OyyHliz3ko",
    "cL_XkTgmz3k1",
    "QFdILTiHz3k1",
    "XmhEkm0nz3k2",
    "bn4Pbjwgz3k4",
    "MJn6VYpQz3k8",
    "JOcri38sz3k8"
   ],
   "name": "Lecture1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
